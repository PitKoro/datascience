{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Телеграмм Чат](./../../img/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Генеративные модели. Автоэнкодеры\n",
    "\n",
    "## Дискриминативные модели\n",
    "Всем Привет! \n",
    "\n",
    "До этого момента, мы с вами работали в основном с дискриминативным моделями,\n",
    "Дискриминативным модели -  это модели которые стремятся выучить распределение `P(y|x)`. То есть они стремятся определить  вероятность принадлежности признака `x ∈ X` к метке класса `y ∈ Y`. Эти модели учатся моделировать границы принятия решений среди классов (например, кошек, собак и тигров). Пытаясь классифицировать признаки `x`, принадлежащий метке класса `y`, дискриминативная модель косвенно изучает определенные особенности набора данных, которые облегчают ее задачу.\n",
    "\n",
    "Если сказать очень простыми словами, то модель пытается выучить сопостовление признака - классу, в следсвтии этого обучения сеть пытается запоннить особенности которые представляют признаки. Например, у автомобиля четыре колеса круглой формы, а светофор вертикальный с тремя круговыми кольцами.\n",
    "\n",
    "Примеры дискриминативных моделей:\n",
    "- Support Vector Machine\n",
    "- Logistic Regression\n",
    "- k-Nearest Neighbour\n",
    "- Random Forest\n",
    "- Deep Neural Network (AlexNet, VGGNet и т.д.)\n",
    "\n",
    "Но мы сегодня не об этом, сегодня мы поговорим и разберем генеративные модели. [Вставка с евы]\n",
    "\n",
    "## Генеративное моделирование\n",
    "Что такое генеративные модели?\n",
    "Генеративные модели, обучаются на понимаение распределение данных. Цель модели состоит в том, чтобы сгенерировать новые признаки из того, что уже было распределено в обучающем наборе. \n",
    "\n",
    "Для разобра примера возьмем датасет KITTI. KITTI - это набор данных об автономном вождении. Допустим мы хотим сгенерировать тоже семантически похожее изображение, то есть изображение из городской среды переводим в сельскую. Сематически данные похожи, есть дорога есть машины и есть дома, но они немного меньше. Но для того, что бы сделать такое, сеть должна понимать основную структуру данных и изучать обобщенние набора данных, небо голубое, пешеходы ходят по тратуарам, а дома бывают разных высот.\n",
    "\n",
    "![](https://waksoft.susu.ru/wp-content/uploads/2021/07/generative-model.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это изобаржение пример, генерации с использование генеративной модели. Цель состоит в том, чтобы обучить модель на такой обучающий выборке с распределением $P_{data}$ и изучить такое распределение, которое: Когда вы выбираете из $P_{model}$ распределения, оно генерирует реалистичные наблюдения, которые представляют истинное распределение. \n",
    "\n",
    "\n",
    "Цель состоит в том, что бы обучить сеть на данных с распределение $P_{data}$. И при использовании распределение $P_{model}$ оно генерировало максимально похожее распределение $P_{data}$. \n",
    "\n",
    "Ну или проще. Цель состоит в том, что бы сеть понимала что из себя представляет котик, что бы выдавать наиболее реалистичных котиков\n",
    "\n",
    "\n",
    "Примеры генеративных моделей:\n",
    "- Наивная байесовская.\n",
    "- Скрытые марковские модели.\n",
    "- Автоэнкодер.\n",
    "- Машины Больцмана.\n",
    "- Вариационный автоэнкодер.\n",
    "- Генеративные состязательные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоэнкодеры\n",
    "\n",
    "\n",
    "Автоэнкодер - это такая сеть которая  сжимают входные данные для представления их в latent-space (скрытое пространство), а затем пытается из этого скрытого пространства восстанавить данные. Цель автоэнкодера — изучить обобщенное скрытое представление набора данных или же получить на выходном слое результат, наиболее близкий к входному. \n",
    "\n",
    "Автоэнкодер состоит из двух блоков:\n",
    "\n",
    "\n",
    "- Енкодер - сжимает входные данные большой размерности в скрытое пространство latent-space \n",
    "- Декодер - наоборот, распаковывая скрытое пространство в исходный многомерный вход.\n",
    "\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&table=block&spaceId=07a1591d-49c2-411b-bfa9-10ca9a9db8f0&width=2000&userId=&cache=v2)\n",
    "\n",
    "\n",
    "Обучение сети будет состоять в том что бы результат сети $\\hat{X}=g(f(x))$ должно быть в том же пространстве что и $X$ и тогда нам надо будет всего лишь сравнить меру схожести $X$ и $\\hat{X}$. А в качестве меры схожести мы можем использовать $MSE$ и тогда у нас получиться мера которая скажет как сильно два набора данных схожи друг на друга $MSE(\\hat{X}, X)$. Чем меньше будет разница тем более схожи изображения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотим как автоэнкодер выглядит в коде "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, input_dim):\n",
    "    super().__init__() \n",
    "    self.encoder = nn. Sequential(\n",
    "        nn.Linear(input_dim, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear (256, 128), \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, input_dim)\n",
    "    )\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(hidden_dim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear (128, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, input_dim)\n",
    "    )\n",
    "\n",
    "    def forward( self, x):\n",
    "        shapes = x.shape\n",
    "        X = x.view(x.size(0) , -1)\n",
    "        return self.decoder(self.encoder(x)).view(*shapes)\n",
    "    \n",
    "    def encode (self, x): \n",
    "        × = x.view(x.size(0), -1)\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядет просто и локанично, пару линейных слоев с функцией активации Relu один декодер сжимает, а энкодер реконструировает скрыторе просранство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужны эти энкодеры?\n",
    "\n",
    "## Понижение размерности\n",
    "Слой энкодера не плохо так понижает размерность, где и как мы можем это использовать? Например для кластеризации больших данных вместо PCA или UMAP \n",
    "\n",
    "\n",
    "### Избавление от шума на изображении.\n",
    "\n",
    "Для обучения мы добавим немного случайного шума в исходные данные, и будем сравнивать не зашумленно изображение с результатом сети. \n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F5c1f09ca-e5e1-4a49-89cd-8e159fcb6b79%2F__2021-04-30__15.07.25.png?id=d9e1e17d-f907-4ccf-a850-6ca3316a2c74&table=block&spaceId=07a1591d-49c2-411b-bfa9-10ca9a9db8f0&width=2000&userId=&cache=v2)\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F5dd49d40-5599-40cd-89ca-e7ca808c48c4%2F__2021-04-30__13.15.15.png?id=6c68621c-eb85-45e4-91d3-180386204505&table=block&spaceId=07a1591d-49c2-411b-bfa9-10ca9a9db8f0&width=2000&userId=&cache=v2)\n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fdc3a176a-7f4d-4c21-99be-e53fb20bdf2a%2FUntitled.png?id=0fb7ec7b-b684-4b53-b193-3d6a78e368c5&table=block&spaceId=07a1591d-49c2-411b-bfa9-10ca9a9db8f0&width=2000&userId=&cache=v2)\n",
    "\n",
    "\n",
    "## Детектирование аномалий \n",
    "\n",
    "Идея заключается в следующем, поскольоку автоэнкодер обучается моделировать распределение признаков в обучающей выборке, то он будет справлятся с реконструкцией наиболее вероятных признаков то есть те признакие которые не являются аномальными. Если в валидационной выборке будет присутствовать аномалия, то кодировшик не сможет ее реконструировать и мы будем иметь большой лосс. То есть аномальными признаками будут те признакие которые выше некоторого порого при расчете лосс функции \n",
    "\n",
    "![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fd3c9104a-21be-4f0b-b27c-3165ea5f9783%2F__2021-04-30__14.33.09.png?id=ef18733f-a6cc-4777-9522-40125bbfa008&table=block&spaceId=07a1591d-49c2-411b-bfa9-10ca9a9db8f0&width=2000&userId=&cache=v2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема классических автоэнкодеров\n",
    "\n",
    "Классические автоэнкодеры обучаются кодировать входные данные и восстанавливать их обратно. \n",
    "\n",
    "Фундаментальная проблема автоэнкодеров заключается в том, что скрытое пространство, в котором они кодируют данные, и в котором лежат их закодированные векторы, не могут быть непрерывными и не позволяют выполнять интерполяцию\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/750/1*-i8cp3ry4XS-05OWPAJLPg.png\" height=\"400\"/>\n",
    "\n",
    "К примеру, обучим автоэнкодер на датасете MNIST и провизуализируем его скрытое пространство в двумерном виде, то увидим четкое формирование кластеров. Это происходит из-за формирование отдельных векторов для каждого класса (типа) изображения, что облегчает декодинг скрытого пространства. \n",
    "\n",
    "Этого достаточно если вам необходимо просто воспроизвести исходные данные. Но если Вы строите генеративную модель, то вам необходимо не просто дублировать изображение, а получить случайное или видоизмененное изображение. которое можно получить из непрерывного срытого пространства. \n",
    "\n",
    "Если срытое пространство имеет разрывы (промежутки между кластерами), и Вы пытаетесь сгенерировать изображение из данного промежутка, то декодер выдаст полный бред, потому что не имеет представления о том как восстановить из этого скрытого пространства. \n",
    "\n",
    "Пример: Мы хотим смешать кроссовки и туфли, в случае классического автоэнкодера вы получили непонятный объект. А в случае вариационного автоэнкодера, мы как раз получим смешанное представление декодера.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Вариационные автоэнкодеры\n",
    "\n",
    "Вариационный автоэнкодер (VAE) как раз умеет создавать непрерывное скрытое пространство позволяя выполнять случайные преобразования и интерполяцию.\n",
    "\n",
    "Непрерывность скрытого пространства создается при помощи энкодера, в VAE он выдаёт не один вектор размера $n$, а два вектора размера $n$ – вектор средних значений $\\mu$ и вектор стандартных отклонений $\\sigma$.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*CiVcrrPmpcB1YGMkTF7hzA.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Вариационные автоэнкодеры формируют параметры вектора длины $n$ из случайных величин $X_i$, причем $i$-е элементы векторов $\\mu$ и $\\sigma$ являются средним и стандартным отклонением $i$-й случайной величины $X_i$. Вместе эти величины образуют n-мерный случайный вектор, который посылается на декодер для восстановления данных:\n",
    " $\\mu$  - это среднее отклонение и  $\\sigma$  - это стандартное отклонение\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*3stEqn8fWIYeeBShlkWAYA.png\" width=\"300\">\n",
    "\n",
    "Эта так называемая стохастическая генерация означает, что даже для одинаковых входных данных результат кодирования будет разным вследствие случайности выбора вектора кодирования:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*96ho7qSyW0nKrLvSoZHOtA.png\" width=\"500\">\n",
    "\n",
    "\n",
    "Среднее значение вектора определяет точку, вблизи которой будет вершина вектора, в то время как стандартное отклонение определяет насколько далеко может находиться вершина от этого среднего.\n",
    "Таким образом, вершина вектора кодирования может лежать внутри $n$-мерного круга (см. рис. выше). Поэтому входному объекту соответствует уже не одна точка в скрытом пространстве, а некоторая непрерывная область. \n",
    "\n",
    "Это позволяет декодеру работать с непрерывным пространством вместо одного единого вектора. По сути мы определяем точку в пространстве, потом некую область и при помощи случайного вектора из нормального. Именно такой способ позволяет добавить в модель инвариантность входных данных. \n",
    "\n",
    "```python\n",
    "class VAE(nn.Module):\n",
    "\tdef __init__(self, inp_dim, latent_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(inp_dim, inp_dim//2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(inp_dim//2, latent_dim * 2)  # mu and log(sigma)\n",
    "\t\t)\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(latent_dim, inp_dim//2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(inp_dim//2, inp_dim)\n",
    "\t\t)\n",
    "\t\tself.latent_dim = latent_dim\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tshapes = x.shape\n",
    "\t\tmu, log_sigma = torch.split(self.encoder(x), self.latent_dim)\n",
    "\t\tkl_loss = self.kl_loss(mu, log_sigma)  # implement it yourself\n",
    "\t\tsample = torch.exp(log_sigma) * torch.randn(shapes[0], self.latent_dim) + mu # часть кода которая отвечает за инвариантность входных данных в декодер\n",
    "\t\tout = self.decoder(sample)\n",
    "\t\treturn out.view(*shapes), kl_loss\n",
    "\t\n",
    "```\n",
    "\n",
    "\n",
    "Теперь модель обладает вариативностью даже в пределах одного вектора кодирования, так как скрытое пространство локально непрерывно, т.е. непрерывно для каждого образца входных данных. В идеальном случае, нам хотелось бы перекрытия этих локальных областей и для образцов входных данных, которые не сильно похожи друг на друга, чтобы производить интерполяцию между ними. Однако, так как нет ограничений на значения, принимаемые векторами $\\mu$  и $\\sigma$, энкодер может быть обучен генерировать сильно отличающиеся µ для разных образцов входных данных, тем самым сильно удаляя их представления друг от друга в скрытом пространстве. Кроме того, энкодер будет минимизировать $\\sigma$ для того, чтобы векторы кодирования не сильно отличались для одного образца. Таким образом, декодер получает данные с малой степенью неопределенности, что позволяет ему эффективно восстанавливать данные из тренировочных сетов, но при этом мы можем не иметь непрерывного пространства:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*xCjoga9IPyNUSiz9E7ao7A.png\" width=\"500\">\n",
    "\n",
    "Мы хотим, чтобы все области в скрытом пространстве были как можно ближе друг к другу, но при этом оставались различимыми как отдельные составляющие. В этом случае мы можем производить гладкую интерполяцию и создавать новые данные на выходе.\n",
    "\n",
    "Для того чтобы достичь этого, в функцию потерь вводится так называемая Kullback–Leibler расходимость (KL divergence). КL расходимость между двумя функциями распределения показывает насколько сильно они отличаются друг от друга. Минимизация KL расходимости означает оптимизацию параметров распределения $\\mu$ и $\\sigma$ таким образом, что они становятся близки к параметрам целевого распределения.\n",
    "\n",
    "\n",
    "$$ \\sum^{n}_{i=1} \\sigma^2_i * \\mu^2_i - \\log{\\sigma_i} -1  $$\n",
    "\n",
    "\n",
    "Для Вариационных Автоэнкодеров KL потери эквивалентны сумме всех KL расходимостей между распределением компонент $X_i \\sim N(\\mu_i^2, \\sigma_i^2)$ в векторе $Х$ и нормальным распределением. Минимум достигается, когда $\\mu_i = 0$ и $\\sigma_i = 1$.\n",
    "\n",
    "Как видно, учёт KL потерь заставляет энкодер помещать каждую отдельную область кодирования в окрестности некоторой точки в скрытом пространстве.\n",
    "\n",
    "При использовании KL потерь области кодирования расположены случайным образом в окрестности выделенной точки в скрытом пространстве со слабым учётом сходства между образцами входных данных. Поэтому декодер не способен извлечь что-либо значащее из этого пространства:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*XdSPoB3rcb7LymviDJBJUg.png)\n",
    "\n",
    "Однако, оптимизируя и энкодер и декодер, мы получаем скрытое пространство, которое отражает схожесть соседних векторов на глобальном уровне, и имеет вид плотно расположенных областей возле начала координат скрытого пространства:\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*BIDBG8MQ9-Kc-knUUrkT3A.png)\n",
    "\n",
    "\n",
    "Достигнутый результат – это компромисс между кластерной природой потерь восстановления, необходимой декодеру, и нашим желанием иметь плотно расположенные векторы при использовании KL потерь. И это здорово, потому что теперь, если вы хотите восстановить входные данные, вы просто выбираете подходящее распределение и посылаете его в декодер. А если вам необходимо произвести интерполяцию, то вы можете смело её осуществить, так как пространство представляет собой гладкое распределение особенностей, что декодер без труда способен обработать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Oct 21 2022, 22:22:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b5619f0dc8a201dce3f980c0c605160d1cdead761ca09207cd8cccaa58881ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
